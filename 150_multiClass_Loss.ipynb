{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no20hZesHZaV"
      },
      "source": [
        "# 150. PyTorch 다중 분류 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLaR1GXEHZaW"
      },
      "source": [
        "## Categorical Crossentropy (범주형 교차 엔트로피)\n",
        "    \n",
        "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\" width=500 />\n",
        "\n",
        "### 🔹 `nn.CrossEntropyLoss` 란?\n",
        "- **PyTorch에서 다중 클래스 분류 문제**를 해결할 때 사용하는 손실 함수입니다.\n",
        "- `nn.LogSoftmax()`와 `nn.NLLLoss()`를 **하나의 클래스로 결합**한 형태입니다.\n",
        "\n",
        "> 즉,  \n",
        "> `nn.CrossEntropyLoss()` = `nn.NLLLoss(nn.LogSoftmax())`\n",
        "\n",
        "---\n",
        "\n",
        "## 주요 수식 및 개념\n",
        "\n",
        "### 1️⃣ **Cross Entropy Loss (교차 엔트로피 손실)**\n",
        "교차 엔트로피 손실은 모델이 예측한 확률 분포와 실제 정답 사이의 차이를 측정하는 방법입니다.\n",
        "\n",
        "$$\n",
        "loss(x, class) = -\\log(\\frac{\\exp(x[class])}{\\sum_{j}{\\exp(x[j])}})\n",
        "$$\n",
        "\n",
        "위 식을 풀어서 보면:\n",
        "\n",
        "- (x[class]) : 정답 클래스의 출력 값 (logit)\n",
        "- $\\sum_{j}{\\exp(x[j])}$ : 모든 클래스에 대해 `softmax`를 적용한 분모 (정규화)\n",
        "- $-x[class] + \\log(\\sum_{j}\\exp(x[j]))$ : 최종적으로 음의 로그 가능도를 계산\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ **LogSoftmax (로그 소프트맥스)**\n",
        "`softmax` 함수에 로그를 적용한 형태입니다.\n",
        "\n",
        "$$\n",
        "LogSoftmax(x_i) = \\log(\\frac{\\exp(x_i)}{\\sum_{j}{\\exp(x_j)}})\n",
        "$$\n",
        "\n",
        "- 왜 LogSoftmax를 사용할까?\n",
        "  - 수학적으로 안정성이 높아 소수점 연산 오류(Underflow)를 방지할 수 있음.\n",
        "  - Softmax 후 로그를 따로 취하는 것보다 더 안전한 방식.\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ **NLLLoss (Negative Log Likelihood Loss, 음의 로그 우도 손실)**\n",
        "`nn.NLLLoss()`는 **확률 값 대신 로그 확률(log probability)을 입력으로 받는 손실 함수**입니다.\n",
        "\n",
        "$$\n",
        "l(x, y) = - \\frac{1}{N} \\sum_{1}^{N}l_n\n",
        "$$\n",
        "\n",
        "- `CrossEntropyLoss`는 내부적으로 `LogSoftmax`를 포함하고 있으므로,  \n",
        "  **입력 값이 logits일 때 바로 사용 가능**합니다.\n",
        "- `NLLLoss`는 로그 확률 값을 입력으로 받으므로,  \n",
        "  **사용할 때 `LogSoftmax`를 별도로 적용해야 함**.\n",
        "\n",
        "---\n",
        "\n",
        "## 결론: `nn.CrossEntropyLoss()` vs `nn.NLLLoss()`\n",
        "| 손실 함수 | 입력 데이터 | 내부 처리 과정 |\n",
        "|-----------|------------|----------------|\n",
        "| **`nn.CrossEntropyLoss()`** | logits (정규화 X) | LogSoftmax + NLLLoss 포함 |\n",
        "| **`nn.NLLLoss()`** | log-probabilities (로그 확률) | LogSoftmax를 먼저 적용해야 함 |\n",
        "\n",
        "> **요약:**  \n",
        "> - `nn.CrossEntropyLoss()`를 사용할 경우, **모델의 마지막 레이어에서 Softmax를 적용하지 않아야 함!**  \n",
        "> - `nn.NLLLoss()`를 사용할 경우, **모델의 마지막 레이어에서 LogSoftmax를 적용해야 함!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqaoUMmlHZaX"
      },
      "source": [
        "### 결론적으로, Pytorch 에서는 Cross Entropy Loss 값의 결과를 얻기 위해 2가지 방식이 존재"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M9Sff9roHZaX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEczqzIDHZaX"
      },
      "source": [
        "### 방법 1\n",
        "### nn.CrossEntropyLoss 함수 사용\n",
        "\n",
        "-  `NNLLoss(Log(Softmax))`\n",
        "\n",
        "$-x[class] + log(\\sum_{j}exp(x[j]))$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 텐서는 모델의 출력으로 볼 수 있으며, 각 요소는 특정 클래스에 속할 확률 또는 점수를 나타냅니다.\n",
        "# 여기서는 10개의 다른 클래스에 대한 점수를 포함하고 있습니다.\n",
        "x = torch.Tensor([[0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.5544]])\n",
        "\n",
        "# 주어진 샘플의 실제 클래스를 1번 클래스로 지정\n",
        "y = torch.LongTensor([1])"
      ],
      "metadata": {
        "id": "3smesO4ZNWMA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hWiDO-2HZaX",
        "outputId": "18a96a9c-d393-4e79-a218-7918f608820b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1438)\n"
          ]
        }
      ],
      "source": [
        "# 크로스 엔트로피 손실 함수 생성 (Softmax + NLLLoss 포함)\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# 예측값 x와 실제 레이블 y에 대해 손실 계산\n",
        "loss = cross_entropy_loss(x, y)\n",
        "\n",
        "# 손실값 출력\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1kiqyWJHZaY"
      },
      "source": [
        "### 방법 2\n",
        "### nn.LogSoftmax + nn.NLLLoss\n",
        "\n",
        "- LogSoftmax : Log + Softmax - 두 함수를 따로 적용한 것 보다 수학적 안정성이 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Wr5IYcHZaY",
        "outputId": "427551ee-8aa4-4653-e804-5298b52e981e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.0506, -2.1438, -2.3095, -1.9505, -2.3757, -2.9019, -2.3928, -2.8012,\n",
              "         -2.1084, -2.3944]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# LogSoftmax 함수 생성 (소프트맥스 적용 후 로그 값 반환)\n",
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "# 입력 x에 대해 로그 소프트맥스 계산 (클래스별 로그 확률)\n",
        "x_log = log_softmax(x)\n",
        "x_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz6x4rupHZaY",
        "outputId": "0eff5a91-22c8-40e1-ffb8-c14944a516d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.1438)\n"
          ]
        }
      ],
      "source": [
        "# torch.nn.NLLLoss()를 인스턴스화하고, 계산된 로그 소프트맥스 값(x_log)과\n",
        "# 실제 레이블(y)을 사용하여 음의 로그 가능도 손실(Negative Log Likelihood Loss)을 계산합니다.\n",
        "loss = nn.NLLLoss()(x_log, y)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dR22avpHZaY"
      },
      "source": [
        "### 사용상의 주의 사항\n",
        "\n",
        "#### nn.CrossEntropyLoss 를 사용할 경우\n",
        "-  nn.CrossEntropyLoss 내에 softmax 함수가 포함되어 있으므로 Neural Network의 마지막 activation 함수로 Softmax를 지정 않고, logit 만 출력한다.  \n",
        "\n",
        "#### nn.NLLLoss 를 사용할 경우\n",
        "- nn.NLLLoss 는 입력으로 확률 분포가 와야 하므로, Neural Network의 마지막 actiovation 함수로 LogSoftmax를 지정 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OmbHJUsHZaZ"
      },
      "source": [
        "### forward method 의 return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4PsDIWY9HZaZ"
      },
      "outputs": [],
      "source": [
        "# nn.Module을 상속받아 사용자 정의 신경망 클래스를 정의합니다.\n",
        "class Ex_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 첫 번째 선형 레이어를 정의합니다. 입력 차원은 2, 출력 차원은 8입니다.\n",
        "        self.linear1 = nn.Linear(2, 8)\n",
        "        # 두 번째 선형 레이어를 정의합니다. 입력 차원은 8, 출력 차원은 3입니다.\n",
        "        self.linear2 = nn.Linear(8, 3)\n",
        "        # LogSoftmax 활성화 함수를 정의합니다.\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward1(self, x):\n",
        "        x = self.linear1(x)  # 입력 x를 첫 번째 선형 레이어에 통과시킵니다.\n",
        "        x = self.linear2(x)  # 두 번째 선형 레이어에 통과시킵니다.\n",
        "        # CrossEntropyLoss를 사용할 경우, 마지막에 log softmax를 사용하지 않습니다.\n",
        "        return x\n",
        "\n",
        "    def forward2(self, x):\n",
        "        x = self.linear1(x)   # 입력 x를 첫 번째 선형 레이어에 통과시킵니다.\n",
        "        x = self.linear2(x)   # 두 번째 선형 레이어에 통과시킵니다\n",
        "        # NLLLoss를 사용할 경우, 마지막에 log softmax를 적용합니다.\n",
        "        x = self.log_softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7_NUNjbqHZaZ"
      },
      "outputs": [],
      "source": [
        "# Ex_NN 모델 인스턴스 생성\n",
        "model = Ex_NN()\n",
        "\n",
        "# 입력 데이터 (3개의 샘플, 각 샘플은 2개의 특성값)\n",
        "x = torch.FloatTensor([[20, 10],\n",
        "                       [10, 30],\n",
        "                       [10, 1]])\n",
        "\n",
        "# 정답 레이블 (각 샘플의 클래스 인덱스)\n",
        "y = torch.tensor([0, 2, 1])  # 첫 번째 샘플은 클래스 0, 두 번째는 2, 세 번째는 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3wy9jKwHZaZ"
      },
      "source": [
        "- CrossEntropyLoss 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZJ3onH6HZaZ",
        "outputId": "674f1887-1669-49be-8e57-7be5ef6fc282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.2550, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()  # 크로스 엔트로피 손실 함수 정의\n",
        "logits = model.forward1(x)    # 모델에 입력 x를 전달해 출력(logits) 계산\n",
        "print(loss(logits, y))        # 손실값 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqpfbuqnHZaZ"
      },
      "source": [
        "- Negative Log Likelihood Loss 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDMl6cFsHZaZ",
        "outputId": "c6ee240d-4379-45a9-81e0-da642002de14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.2550, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss = nn.NLLLoss()  # 음의 로그 가능도 손실 함수 (NLLLoss) 정의\n",
        "prob = model.forward2(x)  # 모델을 통해 LogSoftmax가 적용된 확률(probabilities) 얻기\n",
        "print(loss(prob, y))  # 손실값 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FxVIEexHZaZ"
      },
      "source": [
        "### category 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgl3zLfwHZaa",
        "outputId": "5251b338-2f22-4764-f742-63c78baa3d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-9.0701e+00, -1.3136e-04, -1.1019e+01],\n",
            "        [-1.4418e+01, -5.9605e-07, -2.1607e+01],\n",
            "        [-3.1333e+00, -8.8182e-02, -3.1983e+00]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZaAHK_gHZaa",
        "outputId": "8b134d4a-f78b-44f2-8041-1863fe36f611"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# prob는 모델의 출력 확률 분포를 나타내는 텐서입니다.\n",
        "\n",
        "# torch.argmax 함수는 주어진 차원(axis)에 대해 최대값을 갖는 인덱스를 반환합니다.\n",
        "# 여기서 axis=-1은 텐서의 마지막 차원을 의미합니다. 다중 클래스 분류 문제에서는\n",
        "# 각 샘플의 예측된 클래스 인덱스를 찾는 데 사용됩니다.\n",
        "torch.argmax(prob, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5O6-sDTHZaa",
        "outputId": "20feaf77-ecd2-4d30-e29c-bab59688bc9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([-1.3136e-04, -5.9605e-07, -8.8182e-02], grad_fn=<MaxBackward0>),\n",
              "indices=tensor([1, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# `torch.max` 함수는 주어진 텐서에서 최대 값을 찾습니다.\n",
        "# 이 함수는 두 개의 반환값을 가집니다: 최대 값과 해당 값의 인덱스입니다.\n",
        "# `prob` 변수는 특정 확률 분포 또는 점수가 포함된 텐서를 가정합니다.\n",
        "# `axis=-1` 매개변수는 최대 값을 찾을 차원을 지정합니다.\n",
        "# `-1`은 텐서의 마지막 차원을 의미합니다. 이는 다차원 텐서에서 각 벡터별로 최대 값을 찾는 데 사용됩니다.\n",
        "torch.max(prob, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pvgfQF3EKDis"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}