{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d893e602",
   "metadata": {
    "id": "Qu8nR1-Z0i-u"
   },
   "source": [
    "# TORCHAUDIO 이용 음성 인식\n",
    "\n",
    "**Colab** 이용\n",
    "\n",
    "### **스크립트 요약**: **Torchaudio를 활용한 음성 명령 인식**\n",
    "\n",
    "- 음성 데이터(SpeechCommands)를 이용해 **음성 명령(예: \"yes\", \"no\", \"stop\")**을 인식하는 딥러닝 모델을 학습 및 평가.\n",
    "- **Torchaudio**를 활용하여 오디오 데이터를 전처리하고, CNN 기반 모델(M5)로 학습 수행.\n",
    "\n",
    "#### **데이터 준비**\n",
    "- **SpeechCommands 데이터셋**:\n",
    "  - 약 1초 길이의 음성 데이터(16kHz 샘플링 주파수).\n",
    "  - 명령어 레이블(예: \"yes\", \"no\")와 함께 제공.\n",
    "- 데이터셋을 **훈련(training)**, **검증(validation)**, **테스트(test)** 세트로 분리.\n",
    "\n",
    "#### **전처리**\n",
    "- 음성을 텐서(tensor) 형식으로 변환.\n",
    "- 샘플링 속도를 16kHz에서 **8000Hz로 다운샘플링**하여 처리 속도 향상.\n",
    "- **레이블 인코딩**:\n",
    "  - 텍스트 레이블(예: \"yes\")을 숫자 인덱스로 변환.\n",
    "- 텐서의 패딩 및 CNN 모델 입력 형식으로 변환.\n",
    "\n",
    "#### **모델 정의**\n",
    "- **M5 모델** (1D CNN 기반):\n",
    "  - 입력: 모노 음성 데이터.\n",
    "  - 1D 컨볼루션 레이어와 MaxPooling, Batch Normalization을 결합하여 음성 특징 추출.\n",
    "  - Fully Connected Layer와 Softmax로 명령어를 분류.\n",
    "  - 35개의 클래스(명령어 레이블)로 분류.\n",
    "\n",
    "#### **학습 및 평가**\n",
    "- **Optimizer**: Adam(학습률 초기값: 0.01, 가중치 감쇠 적용).\n",
    "- **Scheduler**: 20 epoch 이후 학습률을 10분의 1로 감소.\n",
    "- 손실 함수: Negative Log-Likelihood Loss.\n",
    "- **훈련 데이터**를 모델에 입력하여 역전파를 통해 가중치를 학습.\n",
    "- 테스트 데이터로 모델 성능 평가(정확도 출력).\n",
    "\n",
    "#### **결과 시각화**\n",
    "- 학습 손실을 epoch별로 그래프로 시각화.\n",
    "- **잘못 분류된 예시**를 찾아 원본 오디오와 예측값 비교.\n",
    "\n",
    "\n",
    "### 주요 출력**\n",
    "\n",
    "- 테스트 정확도:\n",
    "  - 2 Epoch에서 약 65% 이상의 정확도를 달성.\n",
    "  - 21 Epoch에서 85% 이상의 정확도를 달성 예상.\n",
    "- 음성 명령 예측:\n",
    "  - 학습 데이터와 테스트 데이터에서 음성 명령의 예측 결과를 확인.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5031c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204d846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8eb04e",
   "metadata": {
    "id": "MeCNH9Sh0i--"
   },
   "source": [
    "데이터 세트 가져오기\n",
    "---------------------\n",
    "\n",
    "torchaudio를 사용하여 데이터 세트를 다운로드하고 나타냅니다. 여기에서는 서로 다른 사람들이 말한 35 개 명령의 데이터세트 인 SpeechCommands 를 사용 합니다.  SPEECHCOMMANDS는 이 데이터세트의 torch.utils.data.Dataset 버전입니다. 이 데이터세트에서 모든 오디오 파일의 길이는 약 1 초입니다 (따라서 약 16000 time frame 길이 / 16kHz(16,000Hz)).\n",
    "\n",
    "실제 로딩 및 포맷 단계는 데이터 포인트에 액세스 할 때 발생하며 torchaudio는 오디오 파일을 텐서로 변환합니다. 오디오 파일을 직접로드하려면torchaudio.load()를 사용할 수 있습니다. 오디오 파일의 샘플링 주파수 (SpeechCommands의 경우 16kHz)와 함께 새로 생성된 텐서를 포함하는 튜플을 반환합니다.\n",
    "\n",
    "데이터 세트로 돌아가서 training, validation, testing 하위 집합으로 분할하는 subclass를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubsetSC 클래스는 SPEECHCOMMANDS 데이터셋을 상속하여,\n",
    "# 학습, 테스트, 검증용 하위 집합으로 나누기 위한 클래스\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        # 부모 클래스(SPEECHCOMMANDS)를 호출하며 데이터셋을 다운로드함\n",
    "        # 파일에서 리스트를 로드하는 함수\n",
    "        def load_list(filename):\n",
    "            # 파일 경로를 설정\n",
    "            # 파일을 열어 각 줄에 있는 파일 경로를 리스트로 변환\n",
    "        # 검증용 데이터셋을 로드할 경우\n",
    "        # 테스트용 데이터셋을 로드할 경우\n",
    "        # 학습용 데이터셋을 로드할 경우\n",
    "            # 검증 및 테스트 리스트에서 제외할 항목들을 설정\n",
    "            # 학습용 데이터는 검증 및 테스트 데이터에 포함되지 않은 파일들로 구성\n",
    "# 학습과 테스트 데이터를 생성. 이 튜토리얼에서는 검증 데이터를 사용하지 않음\n",
    "# 학습 데이터셋의 첫 번째 항목을 가져옴 (waveform, 샘플링 레이트, 레이블, 화자 ID, 발화 번호)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599217a8",
   "metadata": {
    "id": "lCFjhB3z0i-_"
   },
   "source": [
    "SPEECHCOMMANDS 데이터 세트의 데이터 포인트는 파형 (오디오 신호), 샘플 속도, 발화 (레이블), 화자의 ID, 발화 수로 구성된 튜플입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53441bc0",
   "metadata": {
    "id": "6IKagsojpLBy"
   },
   "source": [
    "- 파형 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23e6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a8a2",
   "metadata": {
    "id": "kwS_JMEr0i_B"
   },
   "source": [
    "데이터 세트에서 사용할 수있는 라벨 목록을 찾아 보겠습니다.  \n",
    "\n",
    "오디오 레이블은 사용자가 말하는 명령입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a2ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels 리스트를 생성하고, train_set에서 세 번째 요소(datapoint[2])를 추출하여 정렬한 결과를 저장\n",
    "# 정렬된 labels 리스트를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6111edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set에서 첫 번째 데이터 포인트를 가져와 필요한 변수들에 할당\n",
    "# - waveform_first: 오디오의 파형 데이터\n",
    "# - sample_rate: 오디오의 샘플링 속도\n",
    "# - label: 오디오 데이터에 대한 레이블 (예: \"yes\", \"no\" 등)\n",
    "# - *_: 나머지 데이터는 사용하지 않음\n",
    "# label(오디오 데이터의 레이블)을 출력\n",
    "# IPython.display 모듈의 Audio 객체를 사용하여 오디오 데이터를 재생\n",
    "# - waveform_first를 NumPy 배열로 변환하여 오디오 데이터를 입력\n",
    "# - rate는 sample_rate로 설정하여 재생 속도를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform_second, sample_rate, label, *_ = train_set[2]\n",
    "# waveform_last, sample_rate, label, *_ = train_set[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7baf6",
   "metadata": {
    "id": "O_CmcCtC0i_H"
   },
   "source": [
    "데이터 포맷\n",
    "-------------------\n",
    "\n",
    "파형의 경우 분류 능력을 너무 많이 잃지 않고 빠른 처리를 위해 오디오를 다운 샘플링합니다.\n",
    "\n",
    "샘플링 주파수(Sampling Rate)란?  \n",
    "    - 샘플링 주파수는 초당 오디오 신호를 얼마나 자주 측정했는지를 나타냅니다.  \n",
    "    - 단위는 **Hz(헤르츠)**이며, 일반적으로 초당 샘플 개수를 나타냅니다.  \n",
    "    - 예: **16,000Hz (16kHz)**는 초당 16,000개의 샘플로 신호를 표현.   **8,000Hz (8kHz)**는 초당 8,000개의 샘플로 신호를 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9555572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 샘플링 속도를 8000Hz로 설정\n",
    "# 원래 샘플링 속도에서 새로운 샘플링 속도로 오디오를 리샘플링하는 변환 설정\n",
    "# 파형(waveform)에 리샘플링 변환을 적용하여 변환된 파형 생성\n",
    "# 리샘플링된 오디오를 새로운 샘플링 속도로 재생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4cc1a",
   "metadata": {
    "id": "dtaal0LO0i_I"
   },
   "source": [
    "레이블 목록의 색인을 사용하여 각 단어를 인코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28670ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # 레이블에서 단어 index를 tensor로 반환\n",
    "def index_to_label(index):\n",
    "    # 레이블의 인덱스에 해당하는 단어를 반환\n",
    "# label --> index --> label 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9873f",
   "metadata": {
    "id": "uQSczk2O0i_J"
   },
   "source": [
    "오디오 녹음 및 발화로 구성된 데이터 포인트 목록을 모델에 대한 두 개의 일괄 텐서로 전환하기 위해 데이터 세트를 일괄적으로 반복할 수 있는 PyTorch DataLoader에서 사용하는 데이터 정렬 함수를 구현합니다.\n",
    "\n",
    "CNN 모델은 주로 입력 형식으로 (batch_size, in_channels, sequence_length)를 요구합니다.\n",
    "이 경우, permute(0, 2, 1)을 사용하면 시퀀스 차원과 특징 차원(feature_size)을 교환하여 CNN이 요구하는 입력 형식 (batch_size, feature_size, sequence_length)로 맞출 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db5267",
   "metadata": {
    "id": "AlH-qeXe0OI0"
   },
   "source": [
    "- batch shape 은 nn.Conv1d의 입력에 맞추어 $(N, C_{\\text{in}}, L)$으로 출력  \n",
    "- N: batch size, C: channels, L: sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ef92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # 오디오 샘플들의 길이가 다양할 수 있으므로 배치 내의 모든 텐서를 동일한 길이로 만듭니다.\n",
    "def collate_fn(batch):\n",
    "    # 리스트로 데이터를 모으고, 레이블을 인덱스로 인코딩\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number 형태의 tuple\n",
    "    # 텐서 리스트를 배치 텐서로 그룹화\n",
    "# GPU를 사용할 경우 관련 설정\n",
    "# 학습 데이터 로더 설정\n",
    "# 테스트 데이터 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader의 첫 번째 배치를 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee83927",
   "metadata": {
    "id": "qFtxhuED0i_L"
   },
   "source": [
    "네트워크 정의\n",
    "------------------\n",
    "\n",
    "원시 오디오 데이터를 처리하기 위해 컨벌루션 신경망을 사용.   \n",
    "일반적으로 더 고급 변환이 오디오 데이터에 적용되지만 CNN을 사용하여 원시 데이터를 정확하게 처리 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f002492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "    def forward(self, x):\n",
    "def count_parameters(model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c51071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513dca36",
   "metadata": {
    "id": "WN74gQYi0i_M"
   },
   "source": [
    "가중치 감소가 0.0001로 설정된 Adam 옵티마이저를 사용. 처음에는 0.01의 학습률로 훈련하지만 scheduler20 에포크 이후 훈련 중에 ``scheduler`` 를 사용 하여 0.001로 줄입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06689f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay: L2 패널티를 적용 (가중치 감쇠)\n",
    "# 학습률 스케줄러 설정\n",
    "# step_size 에포크마다 gamma 비율로 학습률을 감소시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae5cad",
   "metadata": {
    "id": "4GH59aTo0i_N"
   },
   "source": [
    "네트워크 훈련 및 테스트\n",
    "--------------------------------\n",
    "\n",
    "이제 훈련 데이터를 모델에 공급하고 backpropagation 및 optimization 단계를 수행하는 training 함수를 정의해 보겠습니다. 훈련을 위해 우리가 사용할 손실은 negative log-likelihood입니다. 그런 다음 각 Epoch 후에 네트워크를 테스트하여 훈련 중에 정확도가 어떻게 달라지는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "        # device에서 직접 전체 배치에 변환 및 모델 적용\n",
    "        # (batch x 1 x n_output)에 대한 negative log-likelihood 손실 계산\n",
    "        # 학습 통계 출력\n",
    "        # 진행 상황 바 업데이트\n",
    "        # 손실 기록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd9002",
   "metadata": {
    "id": "o_UbMGvc0i_O"
   },
   "source": [
    "이제 training 함수가 있으므로 네트워크 정확도를 테스트하기 위한 함수를 만들어야합니다. 모델을 eval()모드로 설정한 다음 테스트 데이터세트에서 추론을 실행합니다. eval()을 호출하여 네트워크의 모든 모듈에서 훈련 변수를 false로 설정합니다. 배치 정규화 및 드롭 아웃 레이어와 같은 특정 레이어는 훈련 중에 다르게 작동하므로 이 단계는 올바른 결과를 얻는 데 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # 정확한 예측의 개수를 셈\n",
    "def get_likely_index(tensor):\n",
    "    # 배치 내 각 요소에 대해 가장 가능성 높은 라벨 인덱스를 찾음\n",
    "def test(model, epoch):\n",
    "        # device에서 직접 전체 배치에 변환 및 모델 적용\n",
    "        # 진행 상황 바 업데이트\n",
    "    # 에포크별 테스트 결과 출력 (정확도)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539540f4",
   "metadata": {
    "id": "u9uyZ9bi0i_P"
   },
   "source": [
    "이제 네트워크를 훈련하고 테스트 할 수 있습니다. 우리는 10  Epoch 동안 네트워크를 훈련시킨 다음, 학습률을 줄이고 10 Epoch 더 훈련할 것입니다. 훈련 중 정확도가 어떻게 달라지는지 확인하기 위해 각 Epoch 후에 네트워크를 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98921645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환(transform)을 모델 및 데이터와 동일한 장치(device)로 이동\n",
    "# tqdm을 사용하여 진행 상황 표시\n",
    "# 학습 손실을 반복 횟수에 따라 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666d6b3",
   "metadata": {
    "id": "A9wM9Ium0i_Q"
   },
   "source": [
    "네트워크는 2 Epoch 후에 테스트 세트에서 65 % 이상, 21 Epoch 후에 85 % 이상 정확해야합니다. train 세트의 마지막 단어를보고 모델이 어떻게했는지 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # 모델을 사용하여 파형의 레이블을 예측\n",
    "# 학습 세트에서 마지막 항목의 파형(waveform), 샘플링 속도, 발화(utterance) 가져오기\n",
    "# 오디오 재생 (파형을 numpy 배열로 변환하고 샘플링 속도 설정)\n",
    "# 예측된 결과와 실제 발화 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5c4df",
   "metadata": {
    "id": "LIjGOnwg0i_Q"
   },
   "source": [
    "올바르게 분류되지 않은 예를 찾아 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 파형을 예측하여 출력값을 생성\n",
    "    # 예측값이 실제 발화(utterance)와 다를 경우\n",
    "        # 오디오 재생 (파형을 numpy 배열로 변환하고 샘플링 속도를 적용하여 재생)\n",
    "        # 잘못 분류된 데이터 포인트와 예측 결과 출력\n",
    "        # 모든 예시가 올바르게 분류된 경우 출력\n",
    "        # 잘못된 예시가 없을 경우 마지막 데이터 포인트를 출력\n",
    "        # 마지막 데이터 포인트의 오디오 재생\n",
    "        # 마지막 데이터 포인트의 예상 발화와 예측된 결과 출력"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1721bb4304a94e3cabb248c1c41431f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "285ce9556d1b4d8db55b5b989fb04b2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bf5f01483204c75ab287291c12cb679",
       "IPY_MODEL_de3401edbaba43a3b789aff4758a256b",
       "IPY_MODEL_450d5dcf676c4390971a3feaa410ee4f"
      ],
      "layout": "IPY_MODEL_31376c51978e457ab0622355dab0752b"
     }
    },
    "31376c51978e457ab0622355dab0752b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bf5f01483204c75ab287291c12cb679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a953a14915c5404ca04b694c6868ad26",
      "placeholder": "​",
      "style": "IPY_MODEL_bd3b8246a5f74d31919243a4e7dbb423",
      "value": "100%"
     }
    },
    "450d5dcf676c4390971a3feaa410ee4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b4036cda7234492a3487b217f4b287b",
      "placeholder": "​",
      "style": "IPY_MODEL_80e918d9019d4750afa59775d4519fae",
      "value": " 1.9999999999999793/2 [13:25&lt;00:00, 385.48s/it]"
     }
    },
    "7b4036cda7234492a3487b217f4b287b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80e918d9019d4750afa59775d4519fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a953a14915c5404ca04b694c6868ad26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd3b8246a5f74d31919243a4e7dbb423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2cc640dabbc4e699c2fd0e528a309cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de3401edbaba43a3b789aff4758a256b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2cc640dabbc4e699c2fd0e528a309cd",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1721bb4304a94e3cabb248c1c41431f5",
      "value": 1.9999999999999793
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
