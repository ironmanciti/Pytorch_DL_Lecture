{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52085d2e",
   "metadata": {
    "id": "no20hZesHZaV"
   },
   "source": [
    "# 150. PyTorch 다중 분류 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba5b6f",
   "metadata": {
    "id": "xLaR1GXEHZaW"
   },
   "source": [
    "## Categorical Crossentropy\n",
    "    \n",
    "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\" width=500 />\n",
    "\n",
    "##  `nn.CrossEntropyLoss`\n",
    "\n",
    "- `nn.LogSoftmax()` 와 `nn.NLLLoss()` 를 단일 class 로 combine한 것.\n",
    "\n",
    "    즉, `nn.CrossEntropyLoss()` = `nn.NLLLoss(nn.LogSoftmax())`\n",
    "\n",
    "### nn.CrossEntropyLoss\n",
    "\n",
    "$$loss(x, class) = -\\log(\\frac{exp(x[class])}{\\sum_{j}{exp(x[j])}}) = -x[class] + log(\\sum_{j}exp(x[j]))$$\n",
    "\n",
    "### nn.LogSoftmax\n",
    "\n",
    "$$LogSoftmax(x_i) = \\log(\\frac{exp(x_i)}{\\sum_{j}{exp(x_j)}})$$\n",
    "\n",
    "### nn.NLLLoss (Negative Log Likelihood Loss)\n",
    "$$l(x, y) = - \\frac{1}{N} \\sum_{1}^{N}l_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd861d6c",
   "metadata": {
    "id": "zqaoUMmlHZaX"
   },
   "source": [
    "### 결론적으로, Pytorch 에서는 Cross Entropy Loss 값의 결과를 얻기 위해 2가지 방식이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a9f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(logs, targets):\n",
    "    # targets와 동일한 크기의 텐서를 생성하고, 데이터 타입은 float으로 설정합니다.\n",
    "    # 각 타겟 클래스에 대해 로그 확률을 선택합니다.\n",
    "        # logs[i][targets[i]]를 통해, i번째 샘플의 타겟 클래스에 해당하는 로그 확률을 선택합니다.\n",
    "    # 선택된 로그 확률의 합을 구하고, 음수로 변환한 후, 샘플 수로 나누어 평균을 계산합니다.\n",
    "    # 이는 Negative Log Likelihood Loss를 계산하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터\n",
    "# out[i] = logs[i][targets[i]]의 동작 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384cc3c",
   "metadata": {},
   "source": [
    "## 손실 함수 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f795b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 텐서는 모델의 출력으로 볼 수 있으며, 각 요소는 특정 클래스에 속할 확률 또는 점수를 나타냅니다.\n",
    "# 여기서는 10개의 다른 클래스에 대한 점수를 포함하고 있습니다.\n",
    "# 주어진 샘플의 실제 클래스를 1번 클래스로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4f26b",
   "metadata": {
    "id": "fEczqzIDHZaX"
   },
   "source": [
    "### 방법 1\n",
    "### nn.CrossEntropyLoss 함수 사용\n",
    "\n",
    "-  `NNLLoss(Log(Softmax))`\n",
    "\n",
    "$-x[class] + log(\\sum_{j}exp(x[j]))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad158708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.CrossEntropyLoss를 인스턴스화합니다.\n",
    "# 이 손실 함수는 소프트맥스와 NLLLoss(Negative Log Likelihood Loss)를 결합한 것입니다.\n",
    "# 손실 함수를 사용하여 예측값 x와 실제 레이블 y 사이의 크로스 엔트로피 손실을 계산합니다.\n",
    "# CrossEntropyLoss는 내부적으로 x에 대해 소프트맥스 함수를 적용하여 확률 분포를 얻고,\n",
    "# 이 확률 분포와 실제 레이블 y 사이의 NLLLoss를 계산합니다.\n",
    "# 계산된 손실값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50bc8b",
   "metadata": {
    "id": "V1kiqyWJHZaY"
   },
   "source": [
    "### 방법 2\n",
    "### nn.LogSoftmax + NLLLoss\n",
    "\n",
    "- LogSoftmax : Log + Softmax - 두 함수를 따로 적용한 것 보다 수학적 안정성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.LogSoftmax를 인스턴스화합니다.\n",
    "# LogSoftmax는 입력 텐서에 대해 소프트맥스 함수를 적용한 후, 그 결과의 로그 값을 반환\n",
    "# 'dim=1' 매개변수는 소프트맥스 함수가 적용될 차원\n",
    "# 여기서는 각 샘플의 클래스 점수에 대해 소프트맥스 적용\n",
    "# log_softmax를 사용하여 x에 대한 로그 소프트맥스 값을 계산\n",
    "# 이 결과는 각 클래스에 대한 로그 확률을 나타냅니다.\n",
    "# 앞서 정의한 NLLLoss 함수를 사용하여,\n",
    "# 로그 소프트맥스 값(x_log)과 실제 레이블(y) 사이의 음의 로그 가능도 손실(NLL Loss)을 계산합니다.\n",
    "# NLLLoss는 모델의 예측 로그 확률과 실제 레이블 간의 차이를 측정합니다.\n",
    "# 여기서는 모델의 예측 로그 확률(x_log)이 더 정확할수록, 즉 실제 레이블에 해당하는 로그 확률 값이 높을수록 손실이 줄어듭니다.\n",
    "# 계산된 손실값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea11d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.NLLLoss()를 인스턴스화하고, 계산된 로그 소프트맥스 값(x_log)과\n",
    "# 실제 레이블(y)을 사용하여 음의 로그 가능도 손실(Negative Log Likelihood Loss)을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e823d44",
   "metadata": {
    "id": "2dR22avpHZaY"
   },
   "source": [
    "### 사용상의 주의 사항\n",
    "\n",
    "#### nn.CrossEntropyLoss 를 사용할 경우\n",
    "-  nn.CrossEntropyLoss 내에 softmax 함수가 포함되어 있으므로 Neural Network의 마지막 activation 함수로 Softmax를 지정 않고, logit 만 출력한다.  \n",
    "\n",
    "#### nn.NLLLoss 를 사용할 경우\n",
    "- nn.NLLLoss 는 입력으로 확률 분포가 와야 하므로, Neural Network의 마지막 actiovation 함수로 LogSoftmax를 지정 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss 인스턴스를 생성합니다. 이 손실 함수는 소프트맥스 함수를 적용한 뒤,\n",
    "# 크로스 엔트로피 손실을 계산합니다.\n",
    "# LogSoftmax 인스턴스를 생성합니다. 이 함수는 입력 텐서에 대해 로그 소프트맥스 함수를 적용합니다.\n",
    "# NLLLoss(Negative Log Likelihood Loss) 인스턴스를 생성합니다.\n",
    "# 이 손실 함수는 로그 확률에 대한 NLL 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05689a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Label을 나타내는 텐서를 생성합니다.\n",
    "# 좋은 예측값을 나타내는 텐서를 생성합니다.\n",
    "# 이 텐서는 각 샘플에 대해 모델이 예측한 클래스 점수를 담고 있습니다.\n",
    "# 나쁜 예측값을 나타내는 텐서를 생성합니다.\n",
    "# torch.max 함수를 사용하여 y_pred_good의 각 샘플에 대해 가장 높은 점수를 가진 클래스의 인덱스를 얻습니다.\n",
    "# True Label과 같은 결과가 나온 것을 확인합니다.\n",
    "# torch.max 함수를 사용하여 y_pred_bad의 각 샘플에 대해 가장 높은 점수를 가진 클래스의 인덱스를 얻습니다.\n",
    "# True Label 과 다른 결과가 나온 것을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fa131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropy Loss\n",
    "# nn.CrossEntropyLoss()는 내부적으로 로그 소프트맥스와 NLLLoss를 결합한 것으로,\n",
    "# 좋은 예측과 나쁜 예측에 대한 크로스 엔트로피 손실을 계산합니다.\n",
    "# NLLLoss\n",
    "# 먼저 nn.LogSoftmax()를 사용하여 로그 소프트맥스 값을 계산한 후,\n",
    "# nn.NLLLoss()로 네거티브 로그 우도 손실(NLLLoss)을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ffa27",
   "metadata": {
    "id": "5OmbHJUsHZaZ"
   },
   "source": [
    "### forward method 의 return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module을 상속받아 사용자 정의 신경망 클래스를 정의합니다.\n",
    "class Ex_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 첫 번째 선형 레이어를 정의합니다. 입력 차원은 2, 출력 차원은 8입니다.\n",
    "        # 두 번째 선형 레이어를 정의합니다. 입력 차원은 8, 출력 차원은 3입니다.\n",
    "        # LogSoftmax 활성화 함수를 정의합니다.\n",
    "    def forward1(self, x):\n",
    "        # CrossEntropyLoss를 사용할 경우, 마지막에 log softmax를 사용하지 않습니다.\n",
    "    def forward2(self, x):\n",
    "        # NLLLoss를 사용할 경우, 마지막에 log softmax를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752ba26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e335c44",
   "metadata": {
    "id": "L3wy9jKwHZaZ"
   },
   "source": [
    "- CrossEntropyLoss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8a428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "581e6a74",
   "metadata": {
    "id": "YqpfbuqnHZaZ"
   },
   "source": [
    "- Negative Log Likelihood Loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd5dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd321be",
   "metadata": {
    "id": "7FxVIEexHZaZ"
   },
   "source": [
    "### category 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f3876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob는 모델의 출력 확률 분포를 나타내는 텐서입니다.\n",
    "# torch.argmax 함수는 주어진 차원(axis)에 대해 최대값을 갖는 인덱스를 반환합니다.\n",
    "# 여기서 axis=-1은 텐서의 마지막 차원을 의미합니다. 다중 클래스 분류 문제에서는\n",
    "# 각 샘플의 예측된 클래스 인덱스를 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `torch.max` 함수는 주어진 텐서에서 최대 값을 찾습니다.\n",
    "# 이 함수는 두 개의 반환값을 가집니다: 최대 값과 해당 값의 인덱스입니다.\n",
    "# `prob` 변수는 특정 확률 분포 또는 점수가 포함된 텐서를 가정합니다.\n",
    "# `axis=-1` 매개변수는 최대 값을 찾을 차원을 지정합니다.\n",
    "# `-1`은 텐서의 마지막 차원을 의미합니다. 이는 다차원 텐서에서 각 벡터별로 최대 값을 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
