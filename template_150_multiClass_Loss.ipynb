{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7bb82b",
   "metadata": {
    "id": "no20hZesHZaV"
   },
   "source": [
    "# 150. PyTorch 다중 분류 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c760d",
   "metadata": {
    "id": "xLaR1GXEHZaW"
   },
   "source": [
    "## Categorical Crossentropy (범주형 교차 엔트로피)\n",
    "    \n",
    "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\" width=500 />\n",
    "\n",
    "### 🔹 `nn.CrossEntropyLoss` 란?\n",
    "- **PyTorch에서 다중 클래스 분류 문제**를 해결할 때 사용하는 손실 함수입니다.\n",
    "- `nn.LogSoftmax()`와 `nn.NLLLoss()`를 **하나의 클래스로 결합**한 형태입니다.\n",
    "\n",
    "> 즉,  \n",
    "> `nn.CrossEntropyLoss()` = `nn.NLLLoss(nn.LogSoftmax())`\n",
    "\n",
    "---\n",
    "\n",
    "## 주요 수식 및 개념\n",
    "\n",
    "### 1️⃣ **Cross Entropy Loss (교차 엔트로피 손실)**\n",
    "교차 엔트로피 손실은 모델이 예측한 확률 분포와 실제 정답 사이의 차이를 측정하는 방법입니다.\n",
    "\n",
    "$$\n",
    "loss(x, class) = -\\log(\\frac{\\exp(x[class])}{\\sum_{j}{\\exp(x[j])}})\n",
    "$$\n",
    "\n",
    "위 식을 풀어서 보면:\n",
    "\n",
    "- (x[class]) : 정답 클래스의 출력 값 (logit)\n",
    "- $\\sum_{j}{\\exp(x[j])}$ : 모든 클래스에 대해 `softmax`를 적용한 분모 (정규화)\n",
    "- $-x[class] + \\log(\\sum_{j}\\exp(x[j]))$ : 최종적으로 음의 로그 가능도를 계산\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **LogSoftmax (로그 소프트맥스)**\n",
    "`softmax` 함수에 로그를 적용한 형태입니다.\n",
    "\n",
    "$$\n",
    "LogSoftmax(x_i) = \\log(\\frac{\\exp(x_i)}{\\sum_{j}{\\exp(x_j)}})\n",
    "$$\n",
    "\n",
    "- 왜 LogSoftmax를 사용할까?\n",
    "  - 수학적으로 안정성이 높아 소수점 연산 오류(Underflow)를 방지할 수 있음.\n",
    "  - Softmax 후 로그를 따로 취하는 것보다 더 안전한 방식.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **NLLLoss (Negative Log Likelihood Loss, 음의 로그 우도 손실)**\n",
    "`nn.NLLLoss()`는 **확률 값 대신 로그 확률(log probability)을 입력으로 받는 손실 함수**입니다.\n",
    "\n",
    "$$\n",
    "l(x, y) = - \\frac{1}{N} \\sum_{1}^{N}l_n\n",
    "$$\n",
    "\n",
    "- `CrossEntropyLoss`는 내부적으로 `LogSoftmax`를 포함하고 있으므로,  \n",
    "  **입력 값이 logits일 때 바로 사용 가능**합니다.\n",
    "- `NLLLoss`는 로그 확률 값을 입력으로 받으므로,  \n",
    "  **사용할 때 `LogSoftmax`를 별도로 적용해야 함**.\n",
    "\n",
    "---\n",
    "\n",
    "## 결론: `nn.CrossEntropyLoss()` vs `nn.NLLLoss()`\n",
    "| 손실 함수 | 입력 데이터 | 내부 처리 과정 |\n",
    "|-----------|------------|----------------|\n",
    "| **`nn.CrossEntropyLoss()`** | logits (정규화 X) | LogSoftmax + NLLLoss 포함 |\n",
    "| **`nn.NLLLoss()`** | log-probabilities (로그 확률) | LogSoftmax를 먼저 적용해야 함 |\n",
    "\n",
    "> **요약:**  \n",
    "> - `nn.CrossEntropyLoss()`를 사용할 경우, **모델의 마지막 레이어에서 Softmax를 적용하지 않아야 함!**  \n",
    "> - `nn.NLLLoss()`를 사용할 경우, **모델의 마지막 레이어에서 LogSoftmax를 적용해야 함!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23183cd",
   "metadata": {
    "id": "zqaoUMmlHZaX"
   },
   "source": [
    "### 결론적으로, Pytorch 에서는 Cross Entropy Loss 값의 결과를 얻기 위해 2가지 방식이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa9566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83cbfed",
   "metadata": {
    "id": "fEczqzIDHZaX"
   },
   "source": [
    "### 방법 1\n",
    "### nn.CrossEntropyLoss 함수 사용\n",
    "\n",
    "-  `NNLLoss(Log(Softmax))`\n",
    "\n",
    "$-x[class] + log(\\sum_{j}exp(x[j]))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 텐서는 모델의 출력으로 볼 수 있으며, 각 요소는 특정 클래스에 속할 확률 또는 점수를 나타냅니다.\n",
    "# 여기서는 10개의 다른 클래스에 대한 점수를 포함하고 있습니다.\n",
    "# 주어진 샘플의 실제 클래스를 1번 클래스로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크로스 엔트로피 손실 함수 생성 (Softmax + NLLLoss 포함)\n",
    "# 예측값 x와 실제 레이블 y에 대해 손실 계산\n",
    "# 손실값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6463631",
   "metadata": {
    "id": "V1kiqyWJHZaY"
   },
   "source": [
    "### 방법 2\n",
    "### nn.LogSoftmax + nn.NLLLoss\n",
    "\n",
    "- LogSoftmax : Log + Softmax - 두 함수를 따로 적용한 것 보다 수학적 안정성이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogSoftmax 함수 생성 (소프트맥스 적용 후 로그 값 반환)\n",
    "# 입력 x에 대해 로그 소프트맥스 계산 (클래스별 로그 확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f80e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.NLLLoss()를 인스턴스화하고, 계산된 로그 소프트맥스 값(x_log)과\n",
    "# 실제 레이블(y)을 사용하여 음의 로그 가능도 손실(Negative Log Likelihood Loss)을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc69a1",
   "metadata": {
    "id": "2dR22avpHZaY"
   },
   "source": [
    "### 사용상의 주의 사항\n",
    "\n",
    "#### nn.CrossEntropyLoss 를 사용할 경우\n",
    "-  nn.CrossEntropyLoss 내에 softmax 함수가 포함되어 있으므로 Neural Network의 마지막 activation 함수로 Softmax를 지정 않고, logit 만 출력한다.  \n",
    "\n",
    "#### nn.NLLLoss 를 사용할 경우\n",
    "- nn.NLLLoss 는 입력으로 확률 분포가 와야 하므로, Neural Network의 마지막 actiovation 함수로 LogSoftmax를 지정 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef006d",
   "metadata": {
    "id": "5OmbHJUsHZaZ"
   },
   "source": [
    "### forward method 의 return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module을 상속받아 사용자 정의 신경망 클래스를 정의합니다.\n",
    "class Ex_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 첫 번째 선형 레이어를 정의합니다. 입력 차원은 2, 출력 차원은 8입니다.\n",
    "        # 두 번째 선형 레이어를 정의합니다. 입력 차원은 8, 출력 차원은 3입니다.\n",
    "        # LogSoftmax 활성화 함수를 정의합니다.\n",
    "    def forward1(self, x):\n",
    "        # CrossEntropyLoss를 사용할 경우, 마지막에 log softmax를 사용하지 않습니다.\n",
    "    def forward2(self, x):\n",
    "        # NLLLoss를 사용할 경우, 마지막에 log softmax를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex_NN 모델 인스턴스 생성\n",
    "# 입력 데이터 (3개의 샘플, 각 샘플은 2개의 특성값)\n",
    "# 정답 레이블 (각 샘플의 클래스 인덱스)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f5a0c",
   "metadata": {
    "id": "L3wy9jKwHZaZ"
   },
   "source": [
    "- CrossEntropyLoss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7b98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922d5ca5",
   "metadata": {
    "id": "YqpfbuqnHZaZ"
   },
   "source": [
    "- Negative Log Likelihood Loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ac04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e243b50a",
   "metadata": {
    "id": "7FxVIEexHZaZ"
   },
   "source": [
    "### category 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b859b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob는 모델의 출력 확률 분포를 나타내는 텐서입니다.\n",
    "# torch.argmax 함수는 주어진 차원(axis)에 대해 최대값을 갖는 인덱스를 반환합니다.\n",
    "# 여기서 axis=-1은 텐서의 마지막 차원을 의미합니다. 다중 클래스 분류 문제에서는\n",
    "# 각 샘플의 예측된 클래스 인덱스를 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `torch.max` 함수는 주어진 텐서에서 최대 값을 찾습니다.\n",
    "# 이 함수는 두 개의 반환값을 가집니다: 최대 값과 해당 값의 인덱스입니다.\n",
    "# `prob` 변수는 특정 확률 분포 또는 점수가 포함된 텐서를 가정합니다.\n",
    "# `axis=-1` 매개변수는 최대 값을 찾을 차원을 지정합니다.\n",
    "# `-1`은 텐서의 마지막 차원을 의미합니다. 이는 다차원 텐서에서 각 벡터별로 최대 값을 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af23a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
