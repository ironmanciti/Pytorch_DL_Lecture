{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bptrBsIN0aIK"
   },
   "source": [
    "# 120. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FWfvDn9z0aIO",
    "outputId": "a3a096a3-6aec-452d-8bf1-c03d3982a9c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_7QZDgU0aIz"
   },
   "source": [
    "## TORCH.AUTOGRAD를 이용한 자동 미분\n",
    "\n",
    "- autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공   \n",
    "\n",
    "- 신경망을 훈련할 때 가장 자주 사용되는 알고리즘은 역전파이다. 이 알고리즘에서 매개 변수 (모델 가중치)는 주어진 매개 변수에 대한 손실 함수의 기울기에 따라 조정된다.  \n",
    "\n",
    "- 이러한 그래디언트를 계산하기 위해 PyTorch에는 torch.autograd라는 내장 미분 엔진이 있다.\n",
    "\n",
    "- ``.requires_grad`` 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작  \n",
    "\n",
    "- 계산이 완료된 후 ``.backward()`` 를 호출하여 모든 변화도(gradient)를 자동으로 계산  \n",
    "\n",
    "- 이 Tensor의 변화도는 ``.grad`` 속성에 누적  \n",
    "\n",
    "- Tensor가 기록을 추적하는 것을 중단하게 하려면, ``.detach()`` 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지한다.\n",
    "\n",
    "- 도함수를 계산하기 위해서는 Tensor 의 ``.backward()`` 를 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x입력 , 매개변수 , w그리고 손실 함수를 갖는 가장 간단한 단층 신경망을 생각해 보겠습니다 b. PyTorch에서 다음과 같이 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_j214gH0aIz",
    "outputId": "b204636c-d536-4845-f622-4ee040b110a2"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # 입력 텐서 (5차원, 값은 모두 1)\n",
    "y = torch.zeros(3)  # 기대 출력 텐서 (3차원, 값은 모두 0)\n",
    "\n",
    "# 가중치 텐서 (5x3), 학습 대상이므로 requires_grad=True\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "\n",
    "# 편향 텐서 (3차원), 학습 대상이므로 requires_grad=True\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# 선형 연산 z = x @ w + b (행렬곱 후 편향 더함)\n",
    "z = torch.matmul(x, w) + b\n",
    "\n",
    "# Binary Cross Entropy 손실 계산 (로짓 입력 사용)\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 다음과 같은 계산 그래프를 정의합니다 .\n",
    "\n",
    "<img src=\"https://i.imgur.com/Ql8Ff3Z.png\" width=450 />\n",
    "\n",
    "이 네트워크에서 w와 는 최적화해야 할 매개변수b 입니다 . 따라서 해당 변수에 대한 손실 함수의 기울기를 계산할 수 있어야 합니다. 이를 위해 해당 텐서의 속성을 `requires_grad=True`로 설정합니다.  \n",
    "텐서에 적용하여 계산 그래프를 생성하는 함수는 실제로 클래스 객체입니다 Function. 이 객체는 함수를 순방향 으로 계산하는 방법과 역 전파 단계 에서 미분을 계산하는 방법을 알고 있습니다 . 역전파 함수에 대한 참조는 텐서의 속성에 저장됩니다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z에 대한 그래디언트 함수: <AddBackward0 object at 0x000001EAECE137C0>\n",
      "손실 함수에 대한 그래디언트 함수: <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001EAECE13EB0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"z에 대한 그래디언트 함수: {z.grad_fn}\")\n",
    "print(f\"손실 함수에 대한 그래디언트 함수: {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그라디언트 계산 (Gradient Calculation)\n",
    "\n",
    "신경망에서 매개변수의 가중치를 최적화하려면 **손실 함수(Loss Function)** 를 매개변수에 대해 미분해야 합니다. 즉, 다음 미분값을 계산해야 합니다:\n",
    "\n",
    "- ∂Loss / ∂w (가중치에 대한 손실 함수의 변화율)\n",
    "- ∂Loss / ∂b (편향에 대한 손실 함수의 변화율)\n",
    "\n",
    "이러한 미분을 통해 경사하강법(Gradient Descent) 등의 최적화 알고리즘으로 파라미터를 업데이트할 수 있습니다.\n",
    "\n",
    "### PyTorch에서의 순서\n",
    "\n",
    "1. **손실 함수 계산**\n",
    "```python\n",
    "loss = loss_fn(output, target)\n",
    "```\n",
    "\n",
    "2. **역전파 실행 (Backward Propagation)**\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "3. **그라디언트 값 확인**\n",
    "```python\n",
    "print(w.grad)  # 가중치 w에 대한 손실 함수의 미분값\n",
    "print(b.grad)  # 편향 b에 대한 손실 함수의 미분값\n",
    "```\n",
    "\n",
    "여기서 `x`와 `y`는 고정된 입력(input)과 정답(target) 값이며, `w`와 `b`는 학습해야 할 파라미터입니다.\n",
    "\n",
    "이 과정을 통해 파라미터가 손실을 줄이는 방향으로 업데이트됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "morXePXY0aI2",
    "outputId": "ca6e5a11-261f-4c65-edbd-d85c470891d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w 의 기울기 =  tensor([[0.0387, 0.2697, 0.2926],\n",
      "        [0.0387, 0.2697, 0.2926],\n",
      "        [0.0387, 0.2697, 0.2926],\n",
      "        [0.0387, 0.2697, 0.2926],\n",
      "        [0.0387, 0.2697, 0.2926]])\n",
      "b 의 기울기 =  tensor([0.0387, 0.2697, 0.2926])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"w 의 기울기 = \", w.grad)\n",
    "print(\"b 의 기울기 = \", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY2ZFRFB0aI3"
   },
   "source": [
    "### 그라디언트 추적 비활성화\n",
    "\n",
    "기본적으로 requires_grad=True 인 모든 텐서는 계산 기록을 추적하고 기울기 계산을 지원한다. 이를 수행 할 필요가 없는 경우, 즉 네트워크를 통해 순방향 계산만 수행하려는 경우 계산 코드를 torch.inference_mode() 블록으로 둘러 싸서 계산 추적을 중지할 수 있다.\n",
    "\n",
    "그래디언트 추적을 비활성화 해야하는 경우는 다음과 같다.  \n",
    "\n",
    "    - pre-train 된 network 를 fine tuning 하는 경우  \n",
    "    - 기울기를 추적하지 않는 텐서에 대한 계산이 더 효율적이기 때문에 순방향 패스 만 수행할 때 계산 속도를 높인다.\n",
    "    - 주로 모델의 평가(evaluation)나 추론(inference) 단계에서 사용되며, 계산 속도를 높이고 메모리 사용을 줄이는 데 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDfIBKJV0aI3",
    "outputId": "ffe03c91-7cbf-466f-a67f-b265e82cf515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    z = torch.matmul(x, w)+b\n",
    "\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H463KHDp0aI3"
   },
   "source": [
    "- detach()는 특정 텐서에만 적용되어, 해당 텐서가 계산 그래프에서 분리됩니다.  주로 텐서의 일부 값만을 고정시키고 나머지 값들에 대해 그래디언트를 계산하고 싶을 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDJcPSO50aI3",
    "outputId": "ee276277-fb84-4696-e8ef-2194d83b4634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 개의 feature 를 가진 1 layer + sigmoid activation\n",
    "<img src=\"https://i.imgur.com/dQHOqCj.png\" width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before update]\n",
      "w.grad = tensor([0.1328, 0.2656])\n",
      "b.grad = tensor([0.1328])\n",
      "loss = 0.1425\n",
      "\n",
      "[After update]\n",
      "Updated w = tensor([-0.2716, -0.9788])\n",
      "Updated b = tensor([0.2731])\n"
     ]
    }
   ],
   "source": [
    "# 입력 텐서 (x1=1.0, x2=2.0)\n",
    "x = torch.tensor([1.0, 2.0])  # shape: [2]\n",
    "y = torch.tensor([0.0])       # 정답 레이블\n",
    "\n",
    "# 가중치(w1, w2) 및 편향 초기화 (학습 대상)\n",
    "w = torch.randn(2, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 옵티마이저 (경사하강법, 학습률 0.1)\n",
    "optimizer = torch.optim.SGD([w, b], lr=0.1)\n",
    "\n",
    "# ----- 순전파 -----\n",
    "z = torch.matmul(x, w).reshape(-1) + b       # 선형 결합\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)  # BCE 손실 계산\n",
    "\n",
    "# ----- 역전파 -----\n",
    "optimizer.zero_grad()  # 기존 gradient 초기화\n",
    "loss.backward()        # 기울기 계산\n",
    "\n",
    "# 기울기 확인 (업데이트 전)\n",
    "print(f\"[Before update]\")\n",
    "print(f\"w.grad = {w.grad.view(-1)}\")\n",
    "print(f\"b.grad = {b.grad}\")\n",
    "print(f\"loss = {loss.item():.4f}\")\n",
    "\n",
    "# ----- 경사하강법 적용 (1회) -----\n",
    "optimizer.step()  # 파라미터 업데이트\n",
    "\n",
    "# 업데이트된 파라미터 확인\n",
    "print(f\"\\n[After update]\")\n",
    "print(f\"Updated w = {w.data.view(-1)}\")\n",
    "print(f\"Updated b = {b.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1h8Frco0aI_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
